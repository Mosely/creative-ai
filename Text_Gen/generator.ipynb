{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slogan Generator - Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import unidecode\n",
    "import pandas as pd\n",
    "import string\n",
    "import json\n",
    "import requests\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len =  83354\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file = unidecode.unidecode(open('../Data/zero_to_one.txt').read())\n",
    "file_len = len(file)\n",
    "print('file_len = ', file_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EVERY MOMENT IN BUSINESS happens only once. The next Bill Gates will not build an operating system. The next',\n",
       " 'Larry Page or Sergey Brin won’t make a search engine. And the next Mark Zuckerberg won’t create a',\n",
       " 'social network. If you are copying t',\n",
       " 'Blake Masters, took detailed class notes, which circulated far beyond the campus, and in Zero to One',\n",
       " 'I have worked with him to revise the notes for a wider audience. There’s no reason why the future',\n",
       " 'should happen only at Stanford, or in college, or in Silicon Valley.',\n",
       " '1',\n",
       " 'THE CHALLENGE OF THE FUTURE',\n",
       " 'WHENEVER I INTERVIEW someone for a job, I like to ask this question: “What important truth do very few',\n",
       " 'people agree with you on?”']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slogans = []\n",
    "\n",
    "with open('../Data/zero_to_one.txt') as text_file:\n",
    "    for line in text_file:\n",
    "        slogans.append(line.split('\\n')[0])\n",
    "\n",
    "slogans[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking a Random Slogan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'various exciting elements in entirely new ways.” It could even be true. Suppose her idea is to have'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_slogan():\n",
    "    return random.choice(slogans).replace('\"', \"'\")\n",
    "\n",
    "(random_slogan())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *ALTERNATE OPTION* Sample Random Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group of hackers to catch the\n",
      "shark that killed his friend. That has definitely never been done before. But, like the lack of British\n",
      "restaurants in Palo Alto, maybe that's a good thing.\n",
      "The problem wi\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    chunk = file[start_index:end_index]\n",
    "    return chunk\n",
    "\n",
    "print(random_chunk())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.n_layers, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Inputs & Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 10,  11,  12,  39,  40,  41])\n"
     ]
    }
   ],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return (tensor)\n",
    "\n",
    "print(char_tensor('abcDEF'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assemble a pair of input and target tensors for training, from a random chunk. The input will be all characters up to the last, and the target will be all characters from the first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** Because *random_slogan()* returns string with single quotes, ```'str'```, we need to convert it into a double quoted string. That way, we don't encounter issues with strings that contain commas.\n",
    "\n",
    "There is no way in Python to do this, but by using **json**, we can output a double quoted string from the argument we pass in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_set():    \n",
    "    chunk = json.dumps(random_chunk())\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Set up the Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "    \n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[p], hidden)\n",
    "        inp = prime_input[-1]\n",
    "        \n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "        \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will set up a helper function to keep track of time during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the main training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = decoder(inp[c], hidden)\n",
    "        loss += criterion(output, target[c].unsqueeze(0))\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data.item() / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our hyperparameters, instantiate the model, and start straining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 44s (100 5%) 2.4529]\n",
      "Whe xidinade,. In t\\\"nde this Pon oy or'd. Ind ad woao and in the nald the an inn in the cord os bay h \n",
      "\n",
      "[1m 29s (200 10%) 2.1375]\n",
      "Wh_er a cont the jard Irind gingat ong awed tho and dour dand ind of veome conne ant yopde redive the  \n",
      "\n",
      "[2m 16s (300 15%) 2.1533]\n",
      "Wh\u000b",
      ") endet the afuching wa thongeroney--thed and non tivable cunttas 29999997 andere 2999O undetion. I \n",
      "\n",
      "[3m 0s (400 20%) 1.9853]\n",
      "Wh thoughhgan soluil grame you dow eavial the fater on ged that the envan k0ome at moneal co-up in man \n",
      "\n",
      "[3m 44s (500 25%) 1.9082]\n",
      "Whe it worke revertibuing witte plilizated innite prout the you miller p promics Vlived of the youve y \n",
      "\n",
      "[4m 25s (600 30%) 2.1291]\n",
      "Whhe a lot's ar and caper, the produuse. Fod perete's to conome, talle. Cone to vicises lo one and all \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 256\n",
    "n_layers = 3\n",
    "lr = 0.005\n",
    "\n",
    "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set())       \n",
    "    loss_avg += loss\n",
    "    \n",
    "#     print(\"Time this takes: %s\" % (time_since(start)))\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(evaluate('Wh', 100), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Evaluation: Creating Potential Slogans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate('Hello', 100, temperature=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate('Zero ', 100, temperature=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate('Th', 10, temperature=1.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluations(num_predictions=100, predict_len=20, temperature=0.4):\n",
    "    samples = []\n",
    "    \n",
    "    while len(samples) != num_predictions:\n",
    "        prime_str = random.choice(string.ascii_uppercase)\n",
    "        samples.append(evaluate(prime_str, predict_len, temperature))\n",
    "        \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Sentiment Analysis on Potential Slogans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(string):\n",
    "    r = requests.post(\"http://text-processing.com/api/sentiment/\", data={'text': string})\n",
    "    return json.loads(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(num_candidates=5, predict_len=20, temperature=0.8):\n",
    "    candidates = []\n",
    "    # Keep track of how many evaluations we calculate\n",
    "    sentiment_evaluations = 0\n",
    "    # Evaluate potential candidated until we have our desired amount\n",
    "    while len(candidates) != num_candidates:\n",
    "        prime_str = random.choice(string.ascii_uppercase)\n",
    "        sample = evaluate(prime_str, predict_len, temperature)\n",
    "        \n",
    "        # With predicted sample, run through sentiment analysis\n",
    "        sentiment = get_sentiment(sample)\n",
    "        sentiment_evaluations += 1\n",
    "        \n",
    "        # Finalizing candidate if it has a strong enough score\n",
    "        if sentiment['probability']['pos'] > 0.65:\n",
    "            print(sentiment['probability'])\n",
    "            candidates.append(sample)\n",
    "            \n",
    "        # Stop early if calculate too many evaluations (request limit)\n",
    "        if sentiment_evaluations >= 1000:\n",
    "            print(\"too many attempts: \" + str(sentiment_evaluations))\n",
    "            return candidates\n",
    "        \n",
    "    print(\"number of sentiment evaluations done: \" + str(sentiment_evaluations))\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_candidates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL: Exporting the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For use outside of this notebook, we want to save the model and make it easy to import in other files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(decoder.state_dict(), '../MVP/rnn.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
